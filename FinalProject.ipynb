{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "import torch\n",
    "import tqdm\n",
    "import platform\n",
    "import shutil\n",
    "import json\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Michelle\n",
    "data_prefix = \"/media/nvme1/mitquach/ucsc-cse-244-a-2024-fall-final-project/\"\n",
    "model_prefix = \"/media/nvme1/mitquach/ucsc-cse-244-a-2024-fall-final-project/models/\"\n",
    "if platform.node() == 'navi': # Daniel\n",
    "    data_prefix = \"/home/argon/Stuff/CSE244A_project/\"\n",
    "    model_prefix = \"/home/argon/Stuff/CSE244A_project/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.read_csv(os.path.join(data_prefix, 'categories.csv'))\n",
    "train_labels = pd.read_csv(os.path.join(data_prefix, 'train_labeled.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class for labeled images\n",
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)\n",
    "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.data.iloc[idx, 1]  # Assuming the label is in the second column\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define a dataset class for unlabeled images\n",
    "class UnlabeledImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)\n",
    "        img_name = self.image_paths[idx]\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image  # No label returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Adjust mean and std as needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "labeled_dataset = LabeledImageDataset(csv_file=os.path.join(data_prefix,'train_labeled.csv'), root_dir=os.path.join(data_prefix,'train/labeled'), transform=transform)\n",
    "unlabeled_dataset = UnlabeledImageDataset(root_dir=os.path.join(data_prefix,'train/unlabeled'), transform=transform)\n",
    "\n",
    "# Training / validation split\n",
    "val_ratio = 0.1\n",
    "batch_size = 16\n",
    "\n",
    "val_size = int(val_ratio * len(labeled_dataset))\n",
    "train_size = len(labeled_dataset) - val_size\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(12341234)\n",
    "# Generate as indices so we can save them if needed, but I'm not doing that yet - Daniel\n",
    "val_idx, train_idx = torch.utils.data.random_split(torch.arange(len(labeled_dataset)), [val_size, train_size], generator=generator1)\n",
    "train =  torch.utils.data.Subset(labeled_dataset, train_idx)\n",
    "val =  torch.utils.data.Subset(labeled_dataset, val_idx)\n",
    "\n",
    "labeled_train_data = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, persistent_workers=True)\n",
    "labeled_val_data = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display a batch of labeled images with labels\n",
    "def show_labeled_batch(data_loader):\n",
    "    images, labels = next(iter(data_loader))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx in range(min(8, len(images))):\n",
    "        plt.subplot(2, 4, idx + 1)\n",
    "        img = images[idx].permute(1, 2, 0) # Convert from Tensor format\n",
    "        img = img/2 + 0.5 # This roughly un-normalizes them back to a valid range for imshow - Daniel\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'Label: {labels[idx].item()}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to display a batch of unlabeled images\n",
    "def show_unlabeled_batch(data_loader):\n",
    "    images = next(iter(data_loader))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx in range(min(8, len(images))):\n",
    "        plt.subplot(2, 4, idx + 1)\n",
    "        img = images[idx].permute(1, 2, 0) # Convert from Tensor format\n",
    "        img = img/2 + 0.5 # This roughly un-normalizes them back to a valid range for imshow - Daniel\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Unlabeled Image\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display a batch of labeled images\n",
    "print(\"Labeled images:\")\n",
    "show_labeled_batch(labeled_train_data)\n",
    "enumerate(labeled_train_data) # Reset the dataloader https://stackoverflow.com/questions/60311307/how-does-one-reset-the-dataloader-in-pytorch\n",
    "pass\n",
    "\n",
    "# # Display a batch of unlabeled images\n",
    "# print(\"Unlabeled images:\")\n",
    "# show_unlabeled_batch(unlabeled_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/vt_tutorial.html\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the final layer to match the number of classes\n",
    "num_classes = len(categories)  # Adjust to the actual number of classes\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/pdochannel/vision-transformers-in-pytorch-deit/notebook\n",
    "criterion = LabelSmoothingCrossEntropy()\n",
    "# criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr scheduler\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, epoch, model, optimizer, scheduler=None):\n",
    "    checkpoint_dict = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_dict\": model.state_dict(),\n",
    "        \"optimizer_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Consistently name the scheduler key as \"scheduler_dict\"\n",
    "    if scheduler:\n",
    "        checkpoint_dict[\"scheduler_dict\"] = scheduler.state_dict()\n",
    "\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler=None, device='cpu'):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_dict\"])\n",
    "    \n",
    "    # Consistently access the scheduler as \"scheduler_dict\"\n",
    "    if scheduler and \"scheduler_dict\" in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_dict\"])\n",
    "        \n",
    "    return checkpoint[\"epoch\"]\n",
    "\n",
    "class TrainingHistory:\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def save(self, history_path):\n",
    "        with open(history_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"train_loss\": self.train_loss,\n",
    "                \"val_loss\": self.val_loss,\n",
    "                \"train_acc\": self.train_acc,\n",
    "                \"val_acc\": self.val_acc,\n",
    "            }, f)\n",
    "\n",
    "    def load(self, history_path):\n",
    "        with open(history_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            hist = json.load(f)\n",
    "        self.train_loss = hist[\"train_loss\"]\n",
    "        self.val_loss = hist[\"val_loss\"]\n",
    "        self.train_acc = hist[\"train_acc\"]\n",
    "        self.val_acc = hist[\"val_acc\"]\n",
    "    \n",
    "    def append(self, train_loss, val_loss, train_acc, val_acc):\n",
    "        self.train_loss.append(train_loss)\n",
    "        self.val_loss.append(val_loss)\n",
    "        self.train_acc.append(train_acc)\n",
    "        self.val_acc.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(model_prefix, \"michelle_dict\")\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "start_epoch = 0\n",
    "hist = TrainingHistory()\n",
    "\n",
    "checkpoint_path = os.path.join(model_path, \"checkpoint.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_epoch = load_checkpoint(checkpoint_path, model, optimizer, exp_lr_scheduler, device=device)\n",
    "    hist.load(os.path.join(model_path, \"history.json\"))\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in tqdm.tqdm(labeled_train_data, desc=f\"Train ({epoch+1}/{num_epochs})\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(labeled_val_data, desc=f\"Validation ({epoch+1}/{num_epochs})\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Track statistics\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        val_total += labels.size(0)\n",
    "        val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    exp_lr_scheduler.step() # TODO: Check if this is correct\n",
    "    \n",
    "    save_checkpoint(os.path.join(model_path, f\"checkpoint-{epoch}.pth\"), epoch + 1, model, optimizer, exp_lr_scheduler)\n",
    "    shutil.copyfile(os.path.join(model_path, f\"checkpoint-{epoch}.pth\"), os.path.join(model_path, f\"checkpoint.pth\"))\n",
    "\n",
    "    hist.append(train_loss/train_total, val_loss/val_total, train_correct/train_total, val_correct/val_total)\n",
    "    hist.save(os.path.join(model_path, \"history.json\"))\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/train_total:.4f}, Val Loss: {val_loss/val_total:.4f}\")\n",
    "    print(f\"      Train Accuracy: {100 * train_correct/train_total:.2f}%, Val Accuracy: {100 * val_correct/val_total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.plot(range(len(hist.train_loss)), hist.train_loss, label=\"Train\")\n",
    "plt.plot(range(len(hist.val_loss)), hist.val_loss, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.plot(range(len(hist.train_acc)), hist.train_acc, label=\"Train\")\n",
    "plt.plot(range(len(hist.val_acc)), hist.val_acc, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
