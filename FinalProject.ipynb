{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "import torch\n",
    "import tqdm\n",
    "import platform\n",
    "import shutil\n",
    "import json\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://github.com/facebookresearch/deit/blob/7e160fe43f0252d17191b71cbb5826254114ea5b/datasets.py#L108\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Michelle\n",
    "data_prefix = \"/media/nvme1/mitquach/ucsc-cse-244-a-2024-fall-final-project/\"\n",
    "model_prefix = \"/media/nvme1/mitquach/ucsc-cse-244-a-2024-fall-final-project/models/\"\n",
    "if platform.node() == 'navi': # Daniel\n",
    "    data_prefix = \"/home/argon/Stuff/CSE244A_project/\"\n",
    "    model_prefix = \"/home/argon/Stuff/CSE244A_project/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.read_csv(os.path.join(data_prefix, 'categories.csv'))\n",
    "train_labels = pd.read_csv(os.path.join(data_prefix, 'train_labeled.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, epoch, model, optimizer, scheduler=None):\n",
    "    checkpoint_dict = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_dict\": model.state_dict(),\n",
    "        \"optimizer_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Consistently name the scheduler key as \"scheduler_dict\"\n",
    "    if scheduler:\n",
    "        checkpoint_dict[\"scheduler_dict\"] = scheduler.state_dict()\n",
    "\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler=None, device='cpu'):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_dict\"])\n",
    "    \n",
    "    # Consistently access the scheduler as \"scheduler_dict\"\n",
    "    if scheduler and \"scheduler_dict\" in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_dict\"])\n",
    "        \n",
    "    return checkpoint[\"epoch\"]\n",
    "\n",
    "class TrainingHistory:\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def save(self, history_path):\n",
    "        with open(history_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"train_loss\": self.train_loss,\n",
    "                \"val_loss\": self.val_loss,\n",
    "                \"train_acc\": self.train_acc,\n",
    "                \"val_acc\": self.val_acc,\n",
    "            }, f)\n",
    "\n",
    "    def load(self, history_path):\n",
    "        with open(history_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            hist = json.load(f)\n",
    "        self.train_loss = hist[\"train_loss\"]\n",
    "        self.val_loss = hist[\"val_loss\"]\n",
    "        self.train_acc = hist[\"train_acc\"]\n",
    "        self.val_acc = hist[\"val_acc\"]\n",
    "\n",
    "    def append(self, train_loss, val_loss, train_acc, val_acc):\n",
    "        self.train_loss.append(train_loss)\n",
    "        self.val_loss.append(val_loss)\n",
    "        self.train_acc.append(train_acc)\n",
    "        self.val_acc.append(val_acc)\n",
    "\n",
    "    def is_best(self):\n",
    "        \"\"\"Return true if the last epoch added is the best seen so far\"\"\"\n",
    "        return all([self.val_loss[-1] < i for i in self.val_loss[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_csv=None, return_filenames=False, transform=None):\n",
    "        self.label_values = None\n",
    "        self.return_filenames = return_filenames\n",
    "        if label_csv is not None:\n",
    "            csv_data = pd.read_csv(label_csv)\n",
    "            self.filenames = csv_data[\"image\"].tolist()\n",
    "            self.label_values = csv_data[\"id\"].tolist()\n",
    "        else:\n",
    "            self.filenames = os.listdir(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)\n",
    "        img_name = os.path.join(self.root_dir, self.filenames[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        result = [image]\n",
    "\n",
    "        if self.label_values is not None:\n",
    "            result.append(self.label_values[idx])\n",
    "\n",
    "        if self.return_filenames:\n",
    "            result.append(self.filenames[idx])\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))  # Adjust mean and std as needed\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "labeled_dataset = ImageDataset(os.path.join(data_prefix,'train/labeled'), label_csv=os.path.join(data_prefix,'train_labeled.csv'), transform=transform)\n",
    "unlabeled_dataset = ImageDataset(os.path.join(data_prefix,'train/unlabeled'), transform=transform)\n",
    "\n",
    "# Training / validation split\n",
    "val_ratio = 0.1\n",
    "batch_size = 16\n",
    "\n",
    "val_size = int(val_ratio * len(labeled_dataset))\n",
    "train_size = len(labeled_dataset) - val_size\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(12341234)\n",
    "# Generate as indices so we can save them if needed, but I'm not doing that yet - Daniel\n",
    "val_idx, train_idx = torch.utils.data.random_split(torch.arange(len(labeled_dataset)), [val_size, train_size], generator=generator1)\n",
    "train =  torch.utils.data.Subset(labeled_dataset, train_idx)\n",
    "val =  torch.utils.data.Subset(labeled_dataset, val_idx)\n",
    "\n",
    "labeled_train_data = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, persistent_workers=True)\n",
    "labeled_val_data = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display a batch of labeled images with labels\n",
    "def show_labeled_batch(data_loader):\n",
    "    images, labels = next(iter(data_loader))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx in range(min(8, len(images))):\n",
    "        plt.subplot(2, 4, idx + 1)\n",
    "        img = images[idx].permute(1, 2, 0) # Convert from Tensor format\n",
    "        img = img/2 + 0.5 # This roughly un-normalizes them back to a valid range for imshow - Daniel\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'Label: {labels[idx].item()}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to display a batch of unlabeled images\n",
    "def show_unlabeled_batch(data_loader):\n",
    "    images = next(iter(data_loader))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx in range(min(8, len(images))):\n",
    "        plt.subplot(2, 4, idx + 1)\n",
    "        img = images[idx].permute(1, 2, 0) # Convert from Tensor format\n",
    "        img = img/2 + 0.5 # This roughly un-normalizes them back to a valid range for imshow - Daniel\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Unlabeled Image\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display a batch of labeled images\n",
    "print(\"Labeled images:\")\n",
    "show_labeled_batch(labeled_train_data)\n",
    "pass\n",
    "\n",
    "# # Display a batch of unlabeled images\n",
    "# print(\"Unlabeled images:\")\n",
    "# show_unlabeled_batch(unlabeled_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Someday this should probably be some yaml files... - Daniel\n",
    "\n",
    "#TODO: Add batch size to this\n",
    "#TODO: Add model type (e.g. facebookresearch/deit:main)\n",
    "\n",
    "# training_config = {\n",
    "#     \"model_name\":  \"michelle_diet__steplr_3_0.97\",\n",
    "#     \"optimizer_lr\": 0.0001,\n",
    "#     \"optimizer_wd\": 0.0,\n",
    "#     \"scheduler_type\": \"StepLR\",\n",
    "#     \"scheduler_params\": {\"step_size\": 3,\n",
    "#                          \"gamma\": 0.97},\n",
    "# }\n",
    "\n",
    "# training_config = {\n",
    "#     \"model_name\":  \"michelle_diet__plateaulr_0.1_0_0.0\",\n",
    "#     \"optimizer_lr\": 0.0001,\n",
    "#     \"optimizer_wd\": 0.0,\n",
    "#     \"scheduler_type\": \"ReduceLROnPlateau\",\n",
    "#     \"scheduler_params\": {\"factor\": 0.1,\n",
    "#                          \"patience\": 0,\n",
    "#                          \"threshold\": 0.0},\n",
    "# }\n",
    "\n",
    "# training_config = {\n",
    "#     \"model_name\":  \"michelle_diet__freeze11__plateaulr_0.1_0_0.0__wdecay_1en4\",\n",
    "#     \"optimizer_lr\": 0.0001,\n",
    "#     \"optimizer_wd\": 0.0001,\n",
    "#     \"scheduler_type\": \"ReduceLROnPlateau\",\n",
    "#     \"scheduler_params\": {\"factor\": 0.1,\n",
    "#                          \"patience\": 0,\n",
    "#                          \"threshold\": 0.0},\n",
    "#     \"unfreeze_layers\": ['blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias']\n",
    "# }\n",
    "\n",
    "# training_config = {\n",
    "#     \"model_name\":  \"michelle_diet__freeze11__plateaulr_0.1_0_0.0__AdamW_wdecay_1en4\",\n",
    "#     \"optimizer_type\": \"AdamW\",\n",
    "#     \"optimizer_lr\": 0.0001,\n",
    "#     \"optimizer_wd\": 0.0001,\n",
    "#     \"scheduler_type\": \"ReduceLROnPlateau\",\n",
    "#     \"scheduler_params\": {\"factor\": 0.1,\n",
    "#                          \"patience\": 0,\n",
    "#                          \"threshold\": 0.0},\n",
    "#     \"unfreeze_layers\": ['blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias']\n",
    "# }\n",
    "\n",
    "training_config = {\n",
    "    \"model_name\":  \"michelle_diet_imagenetmean__freeze11__plateaulr_0.1_0_0.0__AdamW_wdecay_1en4\",\n",
    "    \"optimizer_type\": \"AdamW\",\n",
    "    \"optimizer_lr\": 0.0001,\n",
    "    \"optimizer_wd\": 0.0001,\n",
    "    \"scheduler_type\": \"ReduceLROnPlateau\",\n",
    "    \"scheduler_params\": {\"factor\": 0.1,\n",
    "                         \"patience\": 0,\n",
    "                         \"threshold\": 0.0},\n",
    "    \"unfreeze_layers\": ['blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/vt_tutorial.html\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the final layer to match the number of classes\n",
    "num_classes = len(categories)  # Adjust to the actual number of classes\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/pdochannel/vision-transformers-in-pytorch-deit/notebook\n",
    "criterion = LabelSmoothingCrossEntropy()\n",
    "# criterion = criterion.to(device)\n",
    "if not \"optimizer_type\" in training_config or training_config[\"optimizer_type\"] == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=training_config[\"optimizer_lr\"], weight_decay=training_config[\"optimizer_wd\"])\n",
    "elif training_config[\"optimizer_type\"] == \"AdamW\":\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=training_config[\"optimizer_lr\"], weight_decay=training_config[\"optimizer_wd\"])\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_by_list(model, unfrozen):\n",
    "    # https://stackoverflow.com/questions/62523912/freeze-certain-layers-of-an-existing-model-in-pytorch\n",
    "    total_unfrozen = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in unfrozen:\n",
    "            total_unfrozen += 1\n",
    "            param.requires_grad_(True)\n",
    "        else:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "if \"unfreeze_layers\" in training_config:\n",
    "    freeze_by_list(model, training_config[\"unfreeze_layers\"])\n",
    "\n",
    "[(i[0], i[1].requires_grad) for i in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr scheduler\n",
    "if \"scheduler_type\" in training_config:\n",
    "    if training_config[\"scheduler_type\"] == \"StepLR\":\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, **training_config[\"scheduler_params\"])\n",
    "    if training_config[\"scheduler_type\"] == \"ReduceLROnPlateau\":\n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, **training_config[\"scheduler_params\"])\n",
    "else:\n",
    "    lr_scheduler = None\n",
    "print(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(model_prefix, training_config[\"model_name\"])\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "start_epoch = 0\n",
    "hist = TrainingHistory()\n",
    "\n",
    "checkpoint_path = os.path.join(model_path, \"checkpoint.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_epoch = load_checkpoint(checkpoint_path, model, optimizer, lr_scheduler, device=device)\n",
    "    hist.load(os.path.join(model_path, \"history.json\"))\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in tqdm.tqdm(labeled_train_data, desc=f\"Train ({epoch+1}/{num_epochs})\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(labeled_val_data, desc=f\"Validation ({epoch+1}/{num_epochs})\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Track statistics\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        val_total += labels.size(0)\n",
    "        val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    if training_config[\"scheduler_type\"] == \"ReduceLROnPlateau\":\n",
    "        lr_scheduler.step(val_loss)\n",
    "    elif lr_scheduler:\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    tmp_checkpoint_path = os.path.join(model_path, f\"checkpoint-{epoch}.pth\")\n",
    "    save_checkpoint(tmp_checkpoint_path, epoch + 1, model, optimizer, lr_scheduler)\n",
    "    shutil.copyfile(tmp_checkpoint_path, os.path.join(model_path, f\"checkpoint.pth\"))\n",
    "    if hist.is_best():\n",
    "        shutil.copyfile(tmp_checkpoint_path, os.path.join(model_path, f\"checkpoint-best.pth\"))\n",
    "    os.unlink(tmp_checkpoint_path)\n",
    "\n",
    "    hist.append(train_loss/train_total, val_loss/val_total, train_correct/train_total, val_correct/val_total)\n",
    "    hist.save(os.path.join(model_path, \"history.json\"))\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/train_total:.6f}, Val Loss: {val_loss/val_total:.6f}\")\n",
    "    print(f\"      Train Accuracy: {100 * train_correct/train_total:.2f}%, Val Accuracy: {100 * val_correct/val_total:.2f}%\")\n",
    "    print(f\"      New LR={[g['lr'] for g in optimizer.param_groups]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.plot(range(len(hist.train_loss)), hist.train_loss, label=\"Train\")\n",
    "plt.plot(range(len(hist.val_loss)), hist.val_loss, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.plot(range(len(hist.train_acc)), hist.train_acc, label=\"Train\")\n",
    "plt.plot(range(len(hist.val_acc)), hist.val_acc, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset(root_dir=os.path.join(data_prefix,'test'), return_filenames=True, transform=transform)\n",
    "test_data = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def create_csv_with_number(base_name, data):\n",
    "    \"\"\"Creates a CSV file with a unique number in the filename if the file already exists.\"\"\"\n",
    "\n",
    "    file_number = 1\n",
    "    file_name = f\"{base_name}.csv\"\n",
    "\n",
    "    while os.path.exists(file_name):\n",
    "        file_name = f\"{base_name}_{file_number}.csv\"\n",
    "        file_number += 1\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"image\", \"id\"])  # Write header\n",
    "        writer.writerows(data)  # Write predictions\n",
    "\n",
    "    print(f\"File '{file_name}' created and results saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, filenames in tqdm.tqdm(test_data, desc=\"Testing\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)  # Get the predicted class IDs\n",
    "\n",
    "        # Store filename and predicted label\n",
    "        results.extend(zip(filenames, predicted.cpu().numpy()))\n",
    "\n",
    "create_csv_with_number(\"test_submission\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
