{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "import torch\n",
    "import tqdm\n",
    "import platform\n",
    "import shutil\n",
    "import json\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import math \n",
    "\n",
    "# https://github.com/facebookresearch/deit/blob/7e160fe43f0252d17191b71cbb5826254114ea5b/datasets.py#L108\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the below paths\n",
    "data_prefix = \"/mnt/c/Users/Michelle Quach/Desktop/UCSC/CSE244A/CSE244A-Final-Project/CSE244A-Final-Project/ucsc-cse-244-a-2024-fall-final-project\"\n",
    "model_prefix = \"/mnt/c/Users/Michelle Quach/Desktop/UCSC/CSE244A/CSE244A-Final-Project/CSE244A-Final-Project/ucsc-cse-244-a-2024-fall-final-project/models/\"\n",
    "if platform.node() == 'navi': # Daniel\n",
    "    data_prefix = \"/home/argon/Stuff/CSE244A_project/\"\n",
    "    model_prefix = \"/home/argon/Stuff/CSE244A_project/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.read_csv(os.path.join(data_prefix, 'categories.csv'))\n",
    "train_labels = pd.read_csv(os.path.join(data_prefix, 'train_labeled.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(conf):\n",
    "    with open(conf[\"model_name\"] + \".yaml\", 'w') as f:\n",
    "        yaml.dump(conf, f)\n",
    "\n",
    "def load_config(conf_name):\n",
    "    with open(conf_name + \".yaml\", 'r') as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Someday this should probably be some yaml files... - Daniel\n",
    "\n",
    "#TODO: Add batch size to this\n",
    "#TODO: Add model type (e.g. facebookresearch/deit:main)\n",
    "\n",
    "# training_config = {\n",
    "#     \"model_name\":  \"michelle_diet__freeze11__plateaulr_0.1_0_0.0__AdamW_wdecay_1en4\",\n",
    "#     \"optimizer_type\": \"AdamW\",\n",
    "#     \"optimizer_lr\": 0.0001,\n",
    "#     \"optimizer_wd\": 0.0001,\n",
    "#     \"scheduler_type\": \"ReduceLROnPlateau\",\n",
    "#     \"scheduler_params\": {\"factor\": 0.1,\n",
    "#                          \"patience\": 0,\n",
    "#                          \"threshold\": 0.0},\n",
    "#     \"unfreeze_layers\": ['blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias']\n",
    "# }\n",
    "# save_config(training_config)\n",
    "# training_config = load_config(\"michelle_diet__freeze11__plateaulr_0.1_0_0.0__AdamW_wdecay_1en4\")\n",
    "\n",
    "# training_config = load_config(\"michelle_diet_imagenetmean__freeze11__plateaulr_0.1_0_0.0__AdamW_wdecay_1en4\")\n",
    "# training_config = load_config(\"michelle_diet_imagenetmean_augmentD__freeze11__explr_1en4_0.8__AdamW_wdecay_1en4\")\n",
    "\n",
    "# training_config = {\n",
    "#     \"model_name\":  \"michelle_diet_imagenetmean_augmentD__freeze10__explr_1en4_0.9__AdamW_wdecay_1en4_batch1024\",\n",
    "#     \"optimizer_type\": \"AdamW\",\n",
    "#     \"optimizer_lr\": 0.0001,\n",
    "#     \"optimizer_wd\": 0.0001,\n",
    "#     \"batch_size\": 1024,\n",
    "#     \"mini_batch_size\": 32,\n",
    "#     \"augment_mode\": \"augmentD\",\n",
    "#     \"scheduler_type\": \"ExponentialLR\",\n",
    "#     \"scheduler_params\": {\"gamma\": 0.8,},\n",
    "#     \"unfreeze_layers\": ['blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias']\n",
    "# }\n",
    "# save_config(training_config)\n",
    "\n",
    "# training_config = {\n",
    "#     \"model_name\":  \"michelle_diet_imagenetmean_augmentD__freeze11__plateaulr_0.1_0_0.0__AdamW_wdecay_1en4_batch1024\",\n",
    "#     \"optimizer_type\": \"AdamW\",\n",
    "#     \"optimizer_lr\": 0.0001,\n",
    "#     \"optimizer_wd\": 0.0001,\n",
    "#     \"batch_size\": 1024,\n",
    "#     \"mini_batch_size\": 32,\n",
    "#     \"augment_mode\": \"augmentD\",\n",
    "#     \"scheduler_type\": \"ReduceLROnPlateau\",\n",
    "#     \"scheduler_params\": {\"factor\": 0.1,\n",
    "#                          \"patience\": 0,\n",
    "#                          \"threshold\": 0.0},\n",
    "#     \"unfreeze_layers\": ['blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias']\n",
    "# }\n",
    "# save_config(training_config)\n",
    "\n",
    "training_config = {\n",
    "    \"model_name\":  \"michelle_diet_imagenetmean_augmentMix__freeze11__plateaulr_0.1_0_0.0__AdamW_wdecay_1en4_batch1024_seed8931444\",\n",
    "    \"optimizer_type\": \"AdamW\",\n",
    "    \"optimizer_lr\": 0.0001,\n",
    "    \"optimizer_wd\": 0.0001,\n",
    "    \"batch_size\": 1024,\n",
    "    \"mini_batch_size\": 32,\n",
    "    \"seed\": 8931444,\n",
    "    \"augment_mode\": \"augmentMix\",\n",
    "    \"scheduler_type\": \"ReduceLROnPlateau\",\n",
    "    \"scheduler_params\": {\"factor\": 0.1,\n",
    "                         \"patience\": 0,\n",
    "                         \"threshold\": 0.0},\n",
    "    \"unfreeze_layers\": ['blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias']\n",
    "}\n",
    "save_config(training_config)\n",
    "\n",
    "\n",
    "\n",
    "# Reproduce the best one?\n",
    "# training_config = load_config(\"michelle_diet_imagenetmean_augmentD__freeze11__explr_1en4_0.8__AdamW_wdecay_1en4\")\n",
    "\n",
    "training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make everything deterministic/reproducable\n",
    "seed = training_config.get(\"seed\", 0)\n",
    "print(\"Setting random seeds to\", seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, epoch, model, optimizer, scheduler=None):\n",
    "    checkpoint_dict = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_dict\": model.state_dict(),\n",
    "        \"optimizer_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Consistently name the scheduler key as \"scheduler_dict\"\n",
    "    if scheduler:\n",
    "        checkpoint_dict[\"scheduler_dict\"] = scheduler.state_dict()\n",
    "\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler=None, device='cpu'):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_dict\"])\n",
    "    \n",
    "    # Consistently access the scheduler as \"scheduler_dict\"\n",
    "    if scheduler and \"scheduler_dict\" in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_dict\"])\n",
    "        \n",
    "    return checkpoint[\"epoch\"]\n",
    "\n",
    "class TrainingHistory:\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def save(self, history_path):\n",
    "        with open(history_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"train_loss\": self.train_loss,\n",
    "                \"val_loss\": self.val_loss,\n",
    "                \"train_acc\": self.train_acc,\n",
    "                \"val_acc\": self.val_acc,\n",
    "            }, f)\n",
    "\n",
    "    def load(self, history_path):\n",
    "        with open(history_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            hist = json.load(f)\n",
    "        self.train_loss = hist[\"train_loss\"]\n",
    "        self.val_loss = hist[\"val_loss\"]\n",
    "        self.train_acc = hist[\"train_acc\"]\n",
    "        self.val_acc = hist[\"val_acc\"]\n",
    "\n",
    "    def append(self, train_loss, val_loss, train_acc, val_acc):\n",
    "        self.train_loss.append(train_loss)\n",
    "        self.val_loss.append(val_loss)\n",
    "        self.train_acc.append(train_acc)\n",
    "        self.val_acc.append(val_acc)\n",
    "\n",
    "    def is_best(self):\n",
    "        \"\"\"Return true if the last epoch added is the best seen so far\"\"\"\n",
    "        return all([self.val_loss[-1] < i for i in self.val_loss[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_csv=None, return_filenames=False, transform=None):\n",
    "        self.label_values = None\n",
    "        self.return_filenames = return_filenames\n",
    "        if label_csv is not None:\n",
    "            csv_data = pd.read_csv(label_csv)\n",
    "            self.filenames = csv_data[\"image\"].tolist()\n",
    "            self.label_values = csv_data[\"id\"].tolist()\n",
    "        else:\n",
    "            self.filenames = os.listdir(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)\n",
    "        img_name = os.path.join(self.root_dir, self.filenames[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        result = [image]\n",
    "\n",
    "        if self.label_values is not None:\n",
    "            result.append(self.label_values[idx])\n",
    "\n",
    "        if self.return_filenames:\n",
    "            result.append(self.filenames[idx])\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))  # Adjust mean and std as needed\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"augment_mode\" in training_config:\n",
    "    augmented_transform = transform\n",
    "else:\n",
    "    if training_config[\"augment_mode\"] == \"augment\" or \\\n",
    "        training_config[\"augment_mode\"] == \"augmentMix\":\n",
    "        # Data augmentation https://www.kaggle.com/code/pdochannel/vision-transformers-in-pytorch-deit/notebook\n",
    "        augmented_transform = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter()]), p=0.25),\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "                    transforms.RandomErasing(p=0.2, value='random')\n",
    "                ])\n",
    "    elif training_config[\"augment_mode\"] == \"augmentD\":\n",
    "        # Less augmented\n",
    "        augmented_transform = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter()]), p=0.25),\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "                ])\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown augmentation\")\n",
    "    augmented_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Applies MixUp augmentation.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"Applies CutMix augmentation.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size, _, h, w = x.size()\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    # Generate bounding box\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(w * cut_rat)\n",
    "    cut_h = int(h * cut_rat)\n",
    "\n",
    "    # Uniformly sample the center of the bounding box\n",
    "    cx = np.random.randint(w)\n",
    "    cy = np.random.randint(h)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, w)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, h)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, w)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, h)\n",
    "\n",
    "    # Apply CutMix\n",
    "    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
    "    y_a, y_b = y, y[index]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (w * h))\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Computes the MixUp/CutMix loss.\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a `worker_init_fn` that configures each dataset copy differently\n",
    "# def worker_init_fn(worker_id):\n",
    "#      worker_info = torch.utils.data.get_worker_info()\n",
    "#      dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "#      overall_start = dataset.start\n",
    "#      overall_end = dataset.end\n",
    "#      # configure the dataset to only process the split workload\n",
    "#      per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
    "#      worker_id = worker_info.id\n",
    "#      dataset.start = overall_start + worker_id * per_worker\n",
    "#      dataset.end = min(dataset.start + per_worker, overall_end)\n",
    "# # TODO: Figure out how to get this to work with our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "train_dataset = ImageDataset(os.path.join(data_prefix,'train/labeled'), label_csv=os.path.join(data_prefix,'train_labeled.csv'), transform=augmented_transform)\n",
    "val_dataset = ImageDataset(os.path.join(data_prefix,'train/labeled'), label_csv=os.path.join(data_prefix,'train_labeled.csv'), transform=transform)\n",
    "# unlabeled_dataset = ImageDataset(os.path.join(data_prefix,'train/unlabeled'), transform=augmented_transform)\n",
    "\n",
    "# Training / validation split\n",
    "val_ratio = 0.1\n",
    "batch_size = training_config.get(\"batch_size\", 16)\n",
    "\n",
    "val_size = int(val_ratio * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(12341234)\n",
    "# Generate as indices so we can save them if needed, but I'm not doing that yet - Daniel\n",
    "val_idx, train_idx = torch.utils.data.random_split(torch.arange(len(train_dataset)), [val_size, train_size], generator=generator1)\n",
    "train =  torch.utils.data.Subset(train_dataset, train_idx)\n",
    "val =  torch.utils.data.Subset(val_dataset, val_idx)\n",
    "\n",
    "labeled_train_data = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, persistent_workers=True)\n",
    "labeled_val_data = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Ensure train/val is well split along class lines\n",
    "# train_classes = [i[1] for i in train]\n",
    "# val_classes = [i[1] for i in val]\n",
    "\n",
    "# print(torch.unique(torch.as_tensor(train_classes),return_counts=True))\n",
    "# print(torch.unique(torch.as_tensor(val_classes),return_counts=True))\n",
    "\n",
    "# torch.unique(torch.as_tensor(train_classes)) == torch.unique(torch.as_tensor(val_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display a batch of labeled images with labels\n",
    "def show_labeled_batch(data_loader):\n",
    "    images, labels = next(iter(data_loader))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx in range(min(8, len(images))):\n",
    "        plt.subplot(2, 4, idx + 1)\n",
    "        img = images[idx].permute(1, 2, 0) # Convert from Tensor format\n",
    "        # https://stackoverflow.com/questions/65676151/how-does-torchvision-transforms-normalize-operate\n",
    "        img = (img * torch.as_tensor(IMAGENET_DEFAULT_STD)) + torch.as_tensor(IMAGENET_DEFAULT_MEAN)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'Label: {labels[idx].item()}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to display a batch of unlabeled images\n",
    "def show_unlabeled_batch(data_loader):\n",
    "    images = next(iter(data_loader))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx in range(min(8, len(images))):\n",
    "        plt.subplot(2, 4, idx + 1)\n",
    "        img = images[idx].permute(1, 2, 0) # Convert from Tensor format\n",
    "        img = (img * torch.as_tensor(IMAGENET_DEFAULT_STD)) + torch.as_tensor(IMAGENET_DEFAULT_MEAN)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Unlabeled Image\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display a batch of labeled images\n",
    "print(\"Labeled images:\")\n",
    "show_labeled_batch(labeled_train_data)\n",
    "pass\n",
    "\n",
    "# # Display a batch of unlabeled images\n",
    "# print(\"Unlabeled images:\")\n",
    "# show_unlabeled_batch(unlabeled_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilled model \n",
    "if training_config.get(\"model\", \"\") == \"deit_distilled\":\n",
    "    model = torch.hub.load('facebookresearch/deit:main', 'deit_base_distilled_patch16_224', pretrained=True)\n",
    "# higher res model\n",
    "elif training_config.get(\"model\", \"\") == \"deit_384\":\n",
    "    model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_384', pretrained=True)\n",
    "else: \n",
    "    # https://pytorch.org/tutorials/beginner/vt_tutorial.html\n",
    "    model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the final layer to match the number of classes\n",
    "num_classes = len(categories)  # Adjust to the actual number of classes\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "if training_config.get(\"model\", \"\") == \"deit_distilled\":\n",
    "    model.head_dist = nn.Linear(model.head_dist.in_features, num_classes) # Modify distilled head for distilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/pdochannel/vision-transformers-in-pytorch-deit/notebook\n",
    "criterion = LabelSmoothingCrossEntropy()\n",
    "# criterion = criterion.to(device)\n",
    "if not \"optimizer_type\" in training_config or training_config[\"optimizer_type\"] == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=training_config[\"optimizer_lr\"], weight_decay=training_config[\"optimizer_wd\"])\n",
    "elif training_config[\"optimizer_type\"] == \"AdamW\":\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=training_config[\"optimizer_lr\"], weight_decay=training_config[\"optimizer_wd\"])\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_by_list(model, unfrozen):\n",
    "    # https://stackoverflow.com/questions/62523912/freeze-certain-layers-of-an-existing-model-in-pytorch\n",
    "    total_unfrozen = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in unfrozen:\n",
    "            total_unfrozen += 1\n",
    "            param.requires_grad_(True)\n",
    "        else:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "if \"unfreeze_layers\" in training_config:\n",
    "    freeze_by_list(model, training_config[\"unfreeze_layers\"])\n",
    "\n",
    "[(i[0], i[1].requires_grad) for i in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr scheduler\n",
    "if \"scheduler_type\" in training_config:\n",
    "    if training_config[\"scheduler_type\"] == \"StepLR\":\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, **training_config[\"scheduler_params\"])\n",
    "    if training_config[\"scheduler_type\"] == \"ExponentialLR\":\n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, **training_config[\"scheduler_params\"])\n",
    "    if training_config[\"scheduler_type\"] == \"ReduceLROnPlateau\":\n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, **training_config[\"scheduler_params\"])\n",
    "    if training_config[\"scheduler_type\"] == \"CosineAnnealingWarmRestarts\":\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, **training_config[\"scheduler_params\"])\n",
    "else:\n",
    "    lr_scheduler = None\n",
    "print(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(model_prefix, training_config[\"model_name\"])\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "start_epoch = 0\n",
    "hist = TrainingHistory()\n",
    "\n",
    "checkpoint_path = os.path.join(model_path, \"checkpoint.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_epoch = load_checkpoint(checkpoint_path, model, optimizer, lr_scheduler, device=device)\n",
    "    hist.load(os.path.join(model_path, \"history.json\"))\n",
    "\n",
    "mini_size = training_config.get(\"mini_batch_size\", batch_size)\n",
    "\n",
    "get_model_output = lambda x: x\n",
    "if training_config.get(\"model\", \"\") == \"deit_distilled\":\n",
    "    get_model_output = lambda x: x[0]\n",
    "\n",
    "use_mixup_augments = training_config.get(\"augment_mode\", \"\") == \"augmentMix\"\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in tqdm.tqdm(labeled_train_data, desc=f\"Train ({epoch+1}/{num_epochs})\"):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_mixup_augments:\n",
    "            # Apply MixUp or CutMix\n",
    "            if np.random.rand() < 0.5:  # Use MixUp\n",
    "                images, y_a, y_b, lam = mixup_data(images, labels, alpha=1.0)\n",
    "            else:  # Use CutMix\n",
    "                images, y_a, y_b, lam = cutmix_data(images, labels, alpha=1.0)\n",
    "\n",
    "        # Break batch into mini-batches that can fit on smaller GPUs\n",
    "        total_size = len(images)\n",
    "        num_chunks = int(math.ceil(total_size / mini_size))\n",
    "        images_chunks = torch.chunk(images, num_chunks)\n",
    "        labels_chunks = torch.chunk(labels, num_chunks)\n",
    "\n",
    "        if use_mixup_augments:\n",
    "            y_a_chunks = torch.chunk(y_a, num_chunks)\n",
    "            y_b_chunks = torch.chunk(y_b, num_chunks)\n",
    "        else:\n",
    "            y_a_chunks = [None]*num_chunks\n",
    "            y_b_chunks = [None]*num_chunks\n",
    "\n",
    "        # Forward pass\n",
    "        for images_chunk, labels_chunk, y_a_chunk, y_b_chunk in zip(images_chunks, labels_chunks, y_a_chunks, y_b_chunks):\n",
    "            images_chunk = images_chunk.to(device)\n",
    "            labels_chunk = labels_chunk.to(device)\n",
    "            outputs = model(images_chunk)\n",
    "\n",
    "            # Compute loss\n",
    "            if use_mixup_augments:\n",
    "                y_a_chunk = y_a_chunk.to(device)\n",
    "                y_b_chunk = y_b_chunk.to(device)\n",
    "                loss = mixup_criterion(criterion, outputs, y_a_chunk, y_b_chunk, lam)\n",
    "            else: \n",
    "                loss = criterion(outputs, labels_chunk)\n",
    "\n",
    "            # Scale average for mini-batching\n",
    "            # FIXME: Is this correct? I'm not sure if the timm.LabelSmoothingCrossEntropy takes the mean like pytorch does\n",
    "            loss = loss * len(images_chunk)/total_size\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            \n",
    "            # Track statistics     \n",
    "            train_loss += loss.item()   \n",
    "            predicted = outputs.argmax(1)\n",
    "            train_total += labels_chunk.size(0)\n",
    "            train_correct += predicted.eq(labels_chunk).sum().item()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(labeled_val_data, desc=f\"Validation ({epoch+1}/{num_epochs})\"):\n",
    "        # Break batch into mini-batches that can fit on smaller GPUs\n",
    "        total_size = len(images)\n",
    "        num_chunks = int(math.ceil(total_size / mini_size))\n",
    "        images_chunks = torch.chunk(images, num_chunks)\n",
    "        labels_chunks = torch.chunk(labels, num_chunks)\n",
    "\n",
    "        # Forward pass\n",
    "        for images_chunk, labels_chunk in zip(images_chunks, labels_chunks):\n",
    "            images_chunk = images_chunk.to(device)\n",
    "            labels_chunk = labels_chunk.to(device)\n",
    "            outputs = model(images_chunk)\n",
    "            loss = criterion(outputs, labels_chunk) * len(images_chunk)/total_size\n",
    "\n",
    "            # Track statistics     \n",
    "            val_loss += loss.item()\n",
    "            predicted = outputs.argmax(1)\n",
    "            val_total += labels_chunk.size(0)\n",
    "            val_correct += predicted.eq(labels_chunk).sum().item()\n",
    "\n",
    "\n",
    "    if training_config[\"scheduler_type\"] == \"ReduceLROnPlateau\":\n",
    "        lr_scheduler.step(val_loss)\n",
    "    elif lr_scheduler:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    hist.append(train_loss/train_total, val_loss/val_total, train_correct/train_total, val_correct/val_total)\n",
    "    \n",
    "    tmp_checkpoint_path = os.path.join(model_path, f\"checkpoint-{epoch}.pth\")\n",
    "    save_checkpoint(tmp_checkpoint_path, epoch + 1, model, optimizer, lr_scheduler)\n",
    "    shutil.copyfile(tmp_checkpoint_path, os.path.join(model_path, f\"checkpoint.pth\"))\n",
    "    if hist.is_best():\n",
    "        shutil.copyfile(tmp_checkpoint_path, os.path.join(model_path, f\"checkpoint-best.pth\"))\n",
    "    os.unlink(tmp_checkpoint_path)\n",
    "\n",
    "    hist.save(os.path.join(model_path, \"history.json\"))\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/train_total:.6f}, Val Loss: {val_loss/val_total:.6f}\")\n",
    "    print(f\"      Train Accuracy: {100 * train_correct/train_total:.2f}%, Val Accuracy: {100 * val_correct/val_total:.2f}%\")\n",
    "    print(f\"      New LR={[g['lr'] for g in optimizer.param_groups]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.plot(range(len(hist.train_loss)), hist.train_loss, label=\"Train\")\n",
    "plt.plot(range(len(hist.val_loss)), hist.val_loss, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.plot(range(len(hist.train_acc)), hist.train_acc, label=\"Train\")\n",
    "plt.plot(range(len(hist.val_acc)), hist.val_acc, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise \"STOP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset(root_dir=os.path.join(data_prefix,'test'), return_filenames=True, transform=transform)\n",
    "test_data = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def create_csv_with_number(base_name, data):\n",
    "    \"\"\"Creates a CSV file with a unique number in the filename if the file already exists.\"\"\"\n",
    "\n",
    "    file_number = 1\n",
    "    file_name = f\"{base_name}.csv\"\n",
    "\n",
    "    while os.path.exists(file_name):\n",
    "        file_name = f\"{base_name}_{file_number}.csv\"\n",
    "        file_number += 1\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"image\", \"id\"])  # Write header\n",
    "        writer.writerows(data)  # Write predictions\n",
    "\n",
    "    print(f\"File '{file_name}' created and results saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = None\n",
    "best_checkpoint_path = os.path.join(model_path, \"checkpoint-best.pth\")\n",
    "best_epoch = load_checkpoint(best_checkpoint_path, model, optimizer, lr_scheduler, device=device)\n",
    "print(best_checkpoint_path)\n",
    "print(\"Best epoch:\", best_epoch)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, filenames in tqdm.tqdm(test_data, desc=\"Testing\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)  # Get the predicted class IDs\n",
    "\n",
    "        # Store filename and predicted label\n",
    "        results.extend(zip(filenames, predicted.cpu().numpy()))\n",
    "\n",
    "create_csv_with_number(\"test_submission\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse244a_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
